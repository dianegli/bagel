{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose: Scrape first 3 pages of Yelp data for every single zipcode in NYC with label of \"bagel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file with all zip codes in NYC\n",
    "\n",
    "zip_nyc = pd.read_csv(\"zipcodes_nyc.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split zipcodes into 10 groups, because we will run the scraper in smaller groups\n",
    "\n",
    "zip_split = np.array_split(zip_nyc.zip_nyc.to_list(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to contain information\n",
    "\n",
    "name = []\n",
    "phone = []\n",
    "rev_count = []\n",
    "price = []\n",
    "food_type = []\n",
    "rat = []\n",
    "add = []\n",
    "town = []\n",
    "zip_search = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works for one url to print a dataframe with the correct columns\n",
    "\n",
    "\n",
    "def get_bagel_data(zips):\n",
    "\n",
    "    for z in zips:\n",
    "        delay = np.random.choice(\n",
    "            [0, 0, 0, 0, 7, 4, 6, 2, 10, 19, 7, 8, 34, 7, 100, 345, 542]\n",
    "        )\n",
    "        print(delay)\n",
    "        print(z)\n",
    "        time.sleep(delay)\n",
    "\n",
    "        for i in [0, 10, 20]:\n",
    "                        url = \"https://api.scrapingdog.com/scrape?api_key={}&url=https://www.yelp.com/search?cflt=bagels&find_loc={}&start={}\".format(os.environ['SCRAPE_API'],z,i)\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "            for item in soup.select(\"[class*=container]\"):\n",
    "                if item.find(\"h4\"):\n",
    "                    name.append(item.find(\"h4\").get_text())\n",
    "                    phone.append(\n",
    "                        item.select(\"[class*=secondaryAttributes]\")[0].get_text()\n",
    "                    )\n",
    "                    try:\n",
    "                        rev_count.append(\n",
    "                            item.select(\"[class*=reviewCount]\")[0].get_text()\n",
    "                        )\n",
    "                    except IndexError:\n",
    "                        rev_count.append(np.nan)\n",
    "                    try:\n",
    "                        price.append(item.select(\"[class*=priceRange]\")[0].get_text())\n",
    "                    except IndexError:\n",
    "                        price.append(np.nan)\n",
    "                    food_type.append(\n",
    "                        item.select(\"[class*=priceCategory]\")[0].get_text()\n",
    "                    )\n",
    "                    try:\n",
    "                        rat.append(item.select(\"[aria-label*=rating]\")[0][\"aria-label\"])\n",
    "                    except IndexError:\n",
    "                        rat.append(np.nan)\n",
    "                    try:\n",
    "                        add.append(item.find(\"address\").get_text())\n",
    "                    except AttributeError:\n",
    "                        add.append(np.nan)\n",
    "                    town.append(\n",
    "                        item.select(\"[class*=margin-b1__09f24__1647o]\")[0].get_text()\n",
    "                    )\n",
    "                    zip_search.append(z)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"name\": name,\n",
    "            \"phone\": phone,\n",
    "            \"review_count\": rev_count,\n",
    "            \"price\": price,\n",
    "            \"food_type\": food_type,\n",
    "            \"rating\": rat,\n",
    "            \"address\": add,\n",
    "            \"town\": town,\n",
    "            \"zip_search\": zip_search,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the bagel as both csv and as a pickle file\n",
    "\n",
    "bagel_nyc_data.to_csv(\n",
    "    date.today().strftime(\"%Y%m%d\") + \"_bagel_nyc_data_10.csv\", index=False\n",
    ")\n",
    "bagel_nyc_data.to_pickle(date.today().strftime(\"%Y%m%d\") + \"_bagel_nyc_data_10.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
